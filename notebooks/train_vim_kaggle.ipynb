{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vision Mamba Training Notebook (Kaggle Version)\n",
                "\n",
                "This notebook is optimized for running on Kaggle. \n",
                "**Prerequisite:** Upload your `data_processed` folder as a Kaggle Dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "import numpy as np\n",
                "from tqdm.notebook import tqdm\n",
                "import sys\n",
                "\n",
                "# ==========================================\n",
                "# KAGGLE SETUP\n",
                "# ==========================================\n",
                "# Install dependencies (Kaggle usually needs these)\n",
                "!pip install -q causal-conv1d>=1.1.0\n",
                "!pip install -q mamba-ssm\n",
                "!pip install -q timm albumentations\n",
                "\n",
                "# Clone Vim Repo if not exists\n",
                "if not os.path.exists('Vim'):\n",
                "    !git clone https://github.com/hustvl/Vim.git\n",
                "\n",
                "# Add paths\n",
                "sys.path.append(os.path.abspath('Vim'))\n",
                "\n",
                "# Copy utils from uploaded dataset or create them inline if they are not in the dataset\n",
                "# Assuming you uploaded the WHOLE project folder as a dataset named 'face-recognition-project'\n",
                "# The path would be /kaggle/input/face-recognition-project/...\n",
                "\n",
                "# For simplicity, let's define the dataset class and transforms INLINE here\n",
                "# to avoid dependency on uploading python files correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# INLINE UTILS (Dataset & Augmentations)\n",
                "# ==========================================\n",
                "import cv2\n",
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "class FaceDataset(Dataset):\n",
                "    def __init__(self, images, labels, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        image = self.images[idx]\n",
                "        label = self.labels[idx]\n",
                "        if image.dtype != np.uint8:\n",
                "            image = image.astype(np.uint8)\n",
                "        if self.transform:\n",
                "            augmented = self.transform(image=image)\n",
                "            image = augmented['image']\n",
                "        else:\n",
                "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
                "        return image, torch.tensor(label, dtype=torch.long)\n",
                "\n",
                "train_transforms = A.Compose([\n",
                "    A.HorizontalFlip(p=0.5),\n",
                "    A.Rotate(limit=10, p=0.5),\n",
                "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
                "    A.RandomResizedCrop(height=224, width=224, scale=(0.9, 1.1), p=0.5),\n",
                "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
                "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
                "    ToTensorV2()\n",
                "])\n",
                "\n",
                "test_transforms = A.Compose([\n",
                "    A.Resize(224, 224),\n",
                "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
                "    ToTensorV2()\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# DATA LOADING (KAGGLE SPECIFIC)\n",
                "# ==========================================\n",
                "# GANTI 'face-recognition-data' DENGAN NAMA DATASET ANDA DI KAGGLE\n",
                "DATA_DIR = \"/kaggle/input/face-recognition-data/data_processed\"\n",
                "TARGET_SIZE = (224, 224)\n",
                "\n",
                "def load_dataset_kaggle():\n",
                "    X = []\n",
                "    y = []\n",
                "    label_map = {}\n",
                "    \n",
                "    if not os.path.exists(DATA_DIR):\n",
                "        print(f\"‚ùå Error: Path {DATA_DIR} tidak ditemukan.\")\n",
                "        print(\"Pastikan Anda sudah Add Data di sidebar kanan Kaggle.\")\n",
                "        return [], [], {}\n",
                "\n",
                "    folders = sorted(os.listdir(DATA_DIR))\n",
                "    for idx, folder in enumerate(folders):\n",
                "        label_map[folder] = idx\n",
                "        folder_path = os.path.join(DATA_DIR, folder)\n",
                "\n",
                "        for filename in os.listdir(folder_path):\n",
                "            if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\")):\n",
                "                img_path = os.path.join(folder_path, filename)\n",
                "                img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
                "                img = cv2.resize(img, TARGET_SIZE)\n",
                "                X.append(img)\n",
                "                y.append(idx)\n",
                "\n",
                "    X = np.array(X)\n",
                "    y = np.array(y)\n",
                "    print(f\"üì¶ Loaded {len(X)} images from {len(label_map)} classes.\")\n",
                "    return X, y, label_map\n",
                "\n",
                "def split_dataset(X, y):\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
                "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
                "    return X_train, X_val, X_test, y_train, y_val, y_test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# CONFIG & MODEL\n",
                "# ==========================================\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 20\n",
                "LEARNING_RATE = 1e-4\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "CHECKPOINT_DIR = \"/kaggle/working/checkpoints\" # Output harus di /kaggle/working\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "def create_vim_model(num_classes):\n",
                "    try:\n",
                "        from vim.models_mamba import VisionMamba\n",
                "        print(\"üêç Menggunakan Vision Mamba (Vim) Model...\")\n",
                "        model = VisionMamba(\n",
                "            img_size=224, \n",
                "            patch_size=16, \n",
                "            embed_dim=192, \n",
                "            depth=24, \n",
                "            rms_norm=True, \n",
                "            residual_in_fp32=True, \n",
                "            fused_add_norm=True, \n",
                "            final_pool_type='mean', \n",
                "            if_abs_pos_embed=True, \n",
                "            if_rope=False, \n",
                "            if_rope_residual=False, \n",
                "            bimamba_type=\"v2\", \n",
                "            if_cls_token=True, \n",
                "            if_devide_out=True, \n",
                "            use_middle_cls_token=True,\n",
                "            num_classes=num_classes\n",
                "        )\n",
                "        return model\n",
                "    except ImportError:\n",
                "        print(\"‚ùå Error: Library 'vim' tidak ditemukan.\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# TRAINING HELPERS\n",
                "# ==========================================\n",
                "def train_one_epoch(model, loader, criterion, optimizer, epoch):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        running_loss += loss.item()\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        pbar.set_postfix({'loss': running_loss/total, 'acc': 100 * correct / total})\n",
                "    return running_loss / len(loader), 100 * correct / total\n",
                "\n",
                "def validate(model, loader, criterion, epoch):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
                "        for images, labels in pbar:\n",
                "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            running_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "            pbar.set_postfix({'loss': running_loss/total, 'acc': 100 * correct / total})\n",
                "    return running_loss / len(loader), 100 * correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# EXECUTION\n",
                "# ==========================================\n",
                "print(\"üöÄ Starting Kaggle Training...\")\n",
                "\n",
                "# 1. Load Data\n",
                "X, y, label_map = load_dataset_kaggle()\n",
                "\n",
                "if len(X) > 0:\n",
                "    num_classes = len(label_map)\n",
                "    print(f\"‚úÖ Detected {num_classes} classes.\")\n",
                "\n",
                "    # 2. Split\n",
                "    X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y)\n",
                "\n",
                "    # 3. Dataloaders\n",
                "    train_dataset = FaceDataset(X_train, y_train, transform=train_transforms)\n",
                "    val_dataset = FaceDataset(X_val, y_val, transform=test_transforms)\n",
                "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "    # 4. Model\n",
                "    model = create_vim_model(num_classes).to(DEVICE)\n",
                "\n",
                "    # 5. Train\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "\n",
                "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "    best_acc = 0.0\n",
                "    \n",
                "    for epoch in range(EPOCHS):\n",
                "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
                "        val_loss, val_acc = validate(model, val_loader, criterion, epoch)\n",
                "        scheduler.step()\n",
                "        \n",
                "        # Store history\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "        \n",
                "        print(f\"Summary: Train Loss {train_loss:.4f} | Val Acc {val_acc:.2f}%\")\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, \"vim_best.pth\"))\n",
                "            print(\"üíæ Saved Best Model\")\n",
                "            \n",
                "    # ==========================================\n",
                "    # VISUALIZATION & METRICS\n",
                "    # ==========================================\n",
                "    import matplotlib.pyplot as plt\n",
                "    import seaborn as sns\n",
                "    from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "    # A. Plot Loss & Accuracy\n",
                "    plt.figure(figsize=(12, 5))\n",
                "\n",
                "    # Loss\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(history['train_loss'], label='Train Loss')\n",
                "    plt.plot(history['val_loss'], label='Val Loss')\n",
                "    plt.title('Loss History')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Loss')\n",
                "    plt.legend()\n",
                "\n",
                "    # Accuracy\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(history['train_acc'], label='Train Acc')\n",
                "    plt.plot(history['val_acc'], label='Val Acc')\n",
                "    plt.title('Accuracy History')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Accuracy (%)')\n",
                "    plt.legend()\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_history.png'))\n",
                "    plt.show()\n",
                "\n",
                "    # B. Confusion Matrix\n",
                "    print(\"üîç Generating Confusion Matrix...\")\n",
                "    model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"vim_best.pth\")))\n",
                "    model.eval()\n",
                "\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
                "            outputs = model(images)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "\n",
                "    # Get Class Names\n",
                "    idx_to_class = {v: k for k, v in label_map.items()}\n",
                "    class_names = [idx_to_class[i] for i in range(len(label_map))]\n",
                "\n",
                "    cm = confusion_matrix(all_labels, all_preds)\n",
                "\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'confusion_matrix.png'))\n",
                "    plt.show()\n",
                "\n",
                "    # C. Classification Report\n",
                "    print(\"\\nüìë Classification Report:\")\n",
                "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
                "\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No data found. Please check dataset path.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}